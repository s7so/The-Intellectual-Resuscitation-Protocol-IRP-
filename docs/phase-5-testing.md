# ğŸ“¦ Phase 5 â€“ Ø§Ø®ØªØ¨Ø§Ø± (Testing)

## ğŸ¯ Ø§Ù„Ù‡Ø¯Ù

Ø¶Ù…Ø§Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ù†ØªØ¬ Ø¹Ø¨Ø± Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙƒØ±Ø§Ø± ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø© Ø°Ø§Øª ØªØºØ·ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ Ù…Ø¹ CI ÙŠØ¶Ù…Ù† Ø£Ù† Ø£ÙŠ ØªØºÙŠÙŠØ± ÙŠÙ…Ø±Ù‘ Ø¨Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¯Ù…Ø¬/Ø§Ù„Ù†Ø´Ø±.

---

## 5.0 â€“ Test Strategy Overview (Ù…Ù„Ø®Øµ Ø³Ø±ÙŠØ¹)

* **Test Pyramid:** Unit (Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©) â†’ Integration â†’ E2E â†’ Manual/Exploratory.
* **Security & Compliance tests:** SAST + DAST + Dependency Scans (SCA) Ø¯Ø§Ø®Ù„ CI.
* **Contract tests:** Consumerâ€‘driven contracts (Pact) Ù„Ù„ÙˆØ§Ø¬Ù‡Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©/Ø§Ù„Ù€ microservices.
* **Mutation testing:** ÙƒÙ€ gate Ù„Ù‚ÙŠØ§Ø³ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù„Ù€ critical services).
* **Continuous testing:** ØªØ´ØºÙŠÙ„ Unit+Integration Ø¹Ù„Ù‰ ÙƒÙ„ pushØŒ E2E + Security nightly or on PR to main.

---

## 5.1 â€“ BDD Scenarios (Gherkin) â€” Ø£Ù…Ø«Ù„Ø© Ø¬Ø§Ù‡Ø²Ø©

Ø¶Ø¹ Ù‡Ø°Ù‡ ÙÙŠ `features/` ÙˆØ§Ø³ØªØ®Ø¯Ù… Cucumber / behave / pytestâ€‘bdd Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ©.

### Feature: Add expense

```gherkin
Feature: Add expense
  As a user
  I want to add a new expense
  So that it appears in my list and totals update

  Scenario: Successful addition
    Given the user is authenticated
    When the user fills the fields with valid data:
      | name   | amount | date       | category |
      | "Ù‚Ù‡ÙˆØ©" | 5.00   | 2025-08-07 | "Food"   |
    And clicks the "Add" button
    Then a success toast "ØªÙ…Øª Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¨Ù†Ø¬Ø§Ø­" is shown
    And the expense appears in the expenses list

  Scenario: Invalid amount
    Given the user is authenticated
    When the user fills the fields with amount "-10"
    Then an error toast "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ù…Ù„Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø¨ØµÙˆØ±Ø© ØµØ­ÙŠØ­Ø©" is shown
    And no new expense is added
```

---

## 5.2 â€“ Unit Tests (best practices)

* Keep unit tests fast, deterministic, isolated from external systems.
* Use test doubles (mocks / stubs / fakes) for DB, external APIs.
* Arrangeâ€‘Actâ€‘Assert structure; clear test names.
* Example (Jest): `tests/services/addExpense.test.js` â€” cover success / validation / repository error.

**Coverage gates:** Configure CI to fail if coverage < 80% (project policy), and perâ€‘module minimal thresholds for critical modules.

---

## 5.3 â€“ Integration Tests

* Test interactions with real DB (use test DB), message brokers or persisting layers.
* Use Docker Compose to bring up dependencies during CI integration job.
* Keep integration tests limited in number and run in a dedicated CI job (longer timeout).

**Example**

* Job `integration-tests` runs: start services via dockerâ€‘compose -f docker-compose.test.yml up -d â†’ run pytest/jest with test DB â†’ teardown.

---

## 5.4 â€“ Endâ€‘toâ€‘End (E2E) Tests

* Target user flows (Add expense, Delete, Filter, Login).
* Use Playwright / Cypress for frontend flows.
* Run E2E in a stable environment (staging mirrored from prod) and back with test data teardown.
* Keep E2E count limited (5â€“10 critical flows) and run on PR to main or nightly.

**Flaky tests mitigation:**

* Retry strategy only at test runner level (e.g., Playwright retries=1) and investigate flakes; do not ignore.
* Track flaky tests in a dashboard and assign owners to fix.

---

## 5.5 â€“ Contract Testing (API compatibility)

* Use Pact or similar: Consumers publish expectations; provider verifies in CI.
* Run provider verification in CI before deploy; fail pipeline if contract broken.

---

## 5.6 â€“ Security & Dependency Scanning in CI

* **SCA (Software Composition Analysis):** Dependabot/GitHub Dependabot or Snyk to scan dependencies.
* **SAST (Static analysis):** Run tools (ESLint + security plugins, bandit for Python, semgrep) in CI on PRs.
* **DAST (Dynamic analysis):** Run OWASP ZAP scans against a deployed test environment nightly or on release candidate.
* **Secrets scanning:** gitâ€‘secrets or GitHub secret scanning in CI.

**Gate policy:** PRs to `main` must pass SCA + SAST checks; high severity issues block merge.

---

## 5.7 â€“ Mutation Testing (measure test quality)

* Tools: Stryker (JS), MutPy (Python), PIT (Java).
* Run mutation tests periodically (weekly or in a dedicated job) and use results to improve tests for critical modules.

---

## 5.8 â€“ Test Data Strategy

* Use fixtures and factories rather than heavy DB seeds.
* Use ephemeral test DB instances per CI job if feasible.
* Keep synthetic test data representative of edge cases (long strings, special chars, timezone boundaries, leap years).

---

## 5.9 â€“ CI Pipeline (suggested GitHub Actions matrix)

**File:** `.github/workflows/ci.yml` (example skeleton)

```yaml
name: CI
on: [push, pull_request]
jobs:
  unit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Node
        uses: actions/setup-node@v3
        with: node-version: '20'
      - run: npm ci
      - run: npm test -- --coverage

  integration:
    needs: unit
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Start services
        run: docker compose -f docker-compose.test.yml up -d
      - run: npm run test:integration

  e2e:
    if: github.event_name == 'pull_request' || github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: npm run test:e2e

  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Snyk
        uses: snyk/actions/node@master
        with:
          args: test
```

**Notes:**

* Separate quick unit job from long integration/E2E/security jobs.
* Use caching (npm, pip) to speed CI.
* Deny merge until `unit` + `security` jobs pass; run slow jobs in gating but optionally allow merge with warning if nonâ€‘critical.

---

## 5.10 â€“ Test Flakiness & Reliability

* Track flaky tests using test result annotations; autoâ€‘label PRs with flaky test indicators.
* Maintain a `flake` dashboard with failure counts and owners.
* Enforce fixing flaky tests within N sprints.

---

## 5.11 â€“ Observability for Tests

* Push test results to a test analytics store (Allure, ReportPortal) for trends.
* Monitor test durations; alert if suite runtime increases beyond threshold.

---

## 5.12 â€“ Linking Tests to SLO / DORA

* Map failing integration/E2E tests types to potential SLO impacts.
* Track deployment failure rate and lead time via CI metrics.

---

## 5.13 â€“ Roles & Ownership (Testing RACI)

| Role         | Responsibility                                        |
| ------------ | ----------------------------------------------------- |
| Developers   | Write Unit + Integration tests; fix failing tests.    |
| QA           | E2E scenarios, exploratory testing, acceptance tests. |
| SRE / DevOps | Maintain CI infra, test environments, data teardown.  |
| Security     | Run SAST/DAST, validate findings, approve releases.   |

---

## 5.14 â€“ Metrics & Gates (example thresholds)

* Unit test coverage: â‰¥ 80%.
* Integration tests pass rate: 100% on PR.
* E2E pass rate: 95%.
* Vulnerabilities: no critical/high unmitigated in dependencies.

---

## 5.15 â€“ Timeâ€‘Box & DoR / DoD

* Writing BDD scenarios: **20â€“30 minutes/story**.
* Unit tests per story: **30â€“60 minutes**.
* Integration/E2E pipeline setup: **1â€“2 days** (initial).

**DoR â€” Phaseâ€‘5**

* âœ… Each User Story has at least one BDD scenario.
* âœ… Unit tests skeleton for the story exists.
* âœ… CI jobs configured for unit + security checks.

**DoD â€” Phaseâ€‘5**

* âœ… All unit tests pass locally and in CI.
* âœ… Integration tests for critical paths pass in CI.
* âœ… E2E tests for critical flows pass in staging and are linked to PR.
* âœ… SAST/SCA scan passed or mitigations documented.

---

# Signâ€‘off

* PO: _______
* Tech Lead: _______
* QA Lead: _______

